# Adversial_Attacks
ğŸš€ FGSM and PGD Attacks on CIFAR-10 Models
ğŸ“Œ Project Overview
This project explores adversarial attacks on deep learning models using Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).
A baseline CNN trained on CIFAR-10 achieved 82% accuracy but was highly vulnerable to attacks.
Targeted attacks were performed to force misclassification (e.g., dog â†’ bird).
An improved ResNet with data augmentation achieved 95% accuracy and showed stronger robustness against adversarial examples.

ğŸ§© Key Features
Implementation of FGSM and PGD attacks.
Targeted adversarial misclassification experiments (e.g., dog â†’ bird, airplane â†’ horse).
Performance comparison between CNN baseline and ResNet + augmentation.
Visualization of adversarial examples and misclassification rates.

ğŸ“Š Results
CNN Baseline (No augmentation):
Accuracy: 82%
FGSM targeted success (dog â†’ bird): 78.2%
PGD targeted success (dog â†’ bird): 100%
ResNet + Data Augmentation:
Accuracy: 95%
FGSM attack on airplane â†’ horse: 3.3% targeted success
PGD attack on airplane â†’ horse: 59.5% targeted success

âš™ï¸ Tech Stack

Python
PyTorch
CIFAR-10 Dataset

